# SciQAEval

SciQAEval evaluates the performance of question-answering systems against scientific publications. It analyzes answers generated by a QA model, computes key NLP metrics, and generates visualizations for a clear understanding of the model's capabilities.

## Features

- **Question Evaluation**: Leverage metrics like semantic similarity, BLEU, ROUGE, and BERTScore to evaluate the quality of answers.
- **Score Visualization**: Visualize the distribution of scores and compare the performance across different metrics with histograms and boxplots.
- **Average Score Calculation**: Compute and output the average for each metric for a quick assessment of overall performance.

## Dependencies

Ensure you have the following Python libraries installed:

- pandas
- matplotlib
- seaborn
- scikit-learn
- sentence_transformers
- nltk
- rouge_score
- bert_score

## Installation

Clone this repository to your local machine using:

```
git clone https://github.com/malsuwailm/SciQAEval.git
```

Install the required dependencies:

```
pip install -r requirements.txt
```

## Usage

Run the questionEvaluationPipeline.py script with the path to your labeled test data in CSV format:

```
python questionEvaluationPipeline.py path/to/labeled_test_data.csv
```

After running the evaluation pipeline, you can visualize the scores using scoreVisualization.py:

```
python scoreVisualization.py
```

## Model Performance



## Contributing

Contributions to SciQAEval are welcome! Please fork the repository and submit a pull request with your proposed changes.

## License

Distributed under the MIT License. See LICENSE for more information.

